{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c7a641f-bc9f-4670-ada8-1c5187e5b74d",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e0bc82b-9901-48af-9df2-ea742ea56afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded\n",
      "Imports completed\n",
      "API Key available: Yes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List\n",
    "from collections import defaultdict\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "from docx import Document\n",
    "from gliner import GLiNER\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Configuration - All parameters in one place\n",
    "class Config:\n",
    "    # Model settings\n",
    "    MODEL_GLINER = \"urchade/gliner_large-v2.1\"\n",
    "    MODEL_NUNER = \"numind/NuNerZero\"\n",
    "    \n",
    "    # Entity labels for extraction\n",
    "    ENTITY_LABELS = {\n",
    "        \"basic\": [\"person\", \"email\"],\n",
    "        \"skills\": [\"skill\", \"technology\", \"tool\", \"programming language\"]\n",
    "    }\n",
    "    \n",
    "    # Confidence thresholds for different entity types\n",
    "    THRESHOLDS = {\n",
    "        \"person\": 0.4,\n",
    "        \"email\": 0.3,\n",
    "        \"skill\": 0.1\n",
    "    }\n",
    "    \n",
    "    # Directories\n",
    "    DATA_DIR = \"./data\"\n",
    "    OUTPUT_DIR = \"./output\"\n",
    "    \n",
    "    # OpenAI API key - set your key here or use environment variable\n",
    "    API_KEY = \"\"  # Replace with your OpenAI API key\n",
    "    \n",
    "    # LLM evaluation schema\n",
    "    EVALUATION_SCHEMA = {\n",
    "        \"name\": \"resume_evaluation\",\n",
    "        \"strict\": True,\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"name_status\": {\n",
    "                    \"type\": \"string\", \n",
    "                    \"enum\": [\"correct\", \"incorrect\", \"partial\"]\n",
    "                },\n",
    "                \"email_status\": {\n",
    "                    \"type\": \"string\", \n",
    "                    \"enum\": [\"correct\", \"incorrect\", \"partial\"]\n",
    "                },\n",
    "                \"skills_status\": {\n",
    "                    \"type\": \"string\", \n",
    "                    \"enum\": [\"correct\", \"incorrect\", \"partial\"]\n",
    "                },\n",
    "                \"extraction_quality\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"excellent\", \"good\", \"fair\", \"poor\"],\n",
    "                    \"description\": \"Overall quality assessment\"\n",
    "                },\n",
    "                \"overall_reasoning\": {\n",
    "                    \"type\": \"string\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"name_status\", \"email_status\", \"skills_status\", \"extraction_quality\", \"overall_reasoning\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"Configuration loaded\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set API key from config or environment\n",
    "if not Config.API_KEY:\n",
    "    Config.API_KEY = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "print(\"Imports completed\")\n",
    "print(f\"API Key available: {'Yes' if Config.API_KEY else 'No (evaluation will be skipped)'}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e9ea66-abaa-4d41-b0cb-0accf9ff69a8",
   "metadata": {},
   "source": [
    "# ResumeParser Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "879f2e0b-6406-4ba8-b542-348e360d9a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResumeParser class defined\n"
     ]
    }
   ],
   "source": [
    "class ResumeParser:\n",
    "    \"\"\"Resume parser using GLiNER and NuNER Zero models.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"Loading models...\")\n",
    "        self.gliner = GLiNER.from_pretrained(Config.MODEL_GLINER)\n",
    "        self.nuner = GLiNER.from_pretrained(Config.MODEL_NUNER)\n",
    "        self.email_regex = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')\n",
    "        print(\"Models loaded successfully\")\n",
    "        \n",
    "    def extract_text(self, file_path: Path) -> str:\n",
    "        \"\"\"Extract text from PDF or DOCX files.\"\"\"\n",
    "        if file_path.suffix.lower() == '.pdf':\n",
    "            doc = fitz.open(file_path)\n",
    "            text = \" \".join([page.get_text() for page in doc])\n",
    "            doc.close()\n",
    "        elif file_path.suffix.lower() == '.docx':\n",
    "            doc = Document(file_path)\n",
    "            text = \" \".join([para.text for para in doc.paragraphs])\n",
    "        else:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "        return text\n",
    "\n",
    "    def merge_adjacent_entities(self, entities: List[Dict], text: str) -> List[Dict]:\n",
    "        \"\"\"Merge adjacent entities for NuNER Zero (handles multi-word entities).\"\"\"\n",
    "        if not entities:\n",
    "            return []\n",
    "        \n",
    "        entities = sorted(entities, key=lambda x: x.get('start', 0))\n",
    "        merged = []\n",
    "        current = entities[0].copy()\n",
    "        \n",
    "        for next_entity in entities[1:]:\n",
    "            if (next_entity['label'] == current['label'] and \n",
    "                next_entity['start'] <= current['end'] + 1):\n",
    "                current['end'] = next_entity['end']\n",
    "                current['text'] = text[current['start']:current['end']].strip()\n",
    "            else:\n",
    "                merged.append(current)\n",
    "                current = next_entity.copy()\n",
    "        merged.append(current)\n",
    "        return merged\n",
    "\n",
    "    def extract_entities(self, text: str) -> Dict[str, List[str]]:\n",
    "        \"\"\"Extract entities using both models with different thresholds.\"\"\"\n",
    "        all_entities = []\n",
    "        \n",
    "        # GLiNER extraction\n",
    "        gliner_basic = self.gliner.predict_entities(\n",
    "            text, Config.ENTITY_LABELS[\"basic\"], threshold=Config.THRESHOLDS[\"person\"]\n",
    "        )\n",
    "        gliner_skills = self.gliner.predict_entities(\n",
    "            text, Config.ENTITY_LABELS[\"skills\"], threshold=Config.THRESHOLDS[\"skill\"]\n",
    "        )\n",
    "        all_entities.extend(gliner_basic + gliner_skills)\n",
    "        \n",
    "        # NuNER Zero extraction (lowercase labels required)\n",
    "        nuner_basic = self.nuner.predict_entities(\n",
    "            text, [l.lower() for l in Config.ENTITY_LABELS[\"basic\"]], \n",
    "            threshold=Config.THRESHOLDS[\"person\"]\n",
    "        )\n",
    "        nuner_skills = self.nuner.predict_entities(\n",
    "            text, [l.lower() for l in Config.ENTITY_LABELS[\"skills\"]], \n",
    "            threshold=Config.THRESHOLDS[\"skill\"]\n",
    "        )\n",
    "        \n",
    "        # Merge adjacent entities for NuNER\n",
    "        nuner_basic = self.merge_adjacent_entities(nuner_basic, text)\n",
    "        nuner_skills = self.merge_adjacent_entities(nuner_skills, text)\n",
    "        all_entities.extend(nuner_basic + nuner_skills)\n",
    "        \n",
    "        # Normalize entity labels\n",
    "        normalized = defaultdict(list)\n",
    "        label_map = {\n",
    "            \"person\": \"name\", \"email\": \"email\", \"skill\": \"skills\",\n",
    "            \"technology\": \"skills\", \"tool\": \"skills\", \"programming language\": \"skills\"\n",
    "        }\n",
    "        \n",
    "        for entity in all_entities:\n",
    "            canonical_label = label_map.get(entity['label'].lower(), entity['label'].lower())\n",
    "            normalized[canonical_label].append(entity['text'].strip())\n",
    "        \n",
    "        return dict(normalized)\n",
    "\n",
    "    def parse_resume(self, file_path: Path) -> Dict:\n",
    "        \"\"\"Parse resume and extract name, email, skills.\"\"\"\n",
    "        text = self.extract_text(file_path)\n",
    "        entities = self.extract_entities(text)\n",
    "        \n",
    "        # Extract name (first valid name)\n",
    "        name = entities.get(\"name\", [\"\"])[0] if entities.get(\"name\") else \"\"\n",
    "        \n",
    "        # Extract email (first valid email)\n",
    "        email = \"\"\n",
    "        email_candidates = entities.get(\"email\", []) + self.email_regex.findall(text)\n",
    "        for candidate in email_candidates:\n",
    "            if \"@\" in candidate and \".\" in candidate.split(\"@\")[-1]:\n",
    "                email = candidate.lower()\n",
    "                break\n",
    "        \n",
    "        # Extract and clean skills\n",
    "        raw_skills = entities.get(\"skills\", [])\n",
    "        skills = []\n",
    "        \n",
    "        for skill in raw_skills:\n",
    "            cleaned = re.sub(r'[^\\w\\s\\+\\-\\.\\#]', '', skill)\n",
    "            if 2 <= len(cleaned) <= 25:\n",
    "                skills.append(cleaned)\n",
    "        \n",
    "        # Remove duplicates while preserving order\n",
    "        unique_skills = list(dict.fromkeys(skills))[:15]\n",
    "        \n",
    "        return {\n",
    "            \"name\": name,\n",
    "            \"email\": email,\n",
    "            \"skills\": unique_skills\n",
    "        }\n",
    "\n",
    "print(\"ResumeParser class defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f8d158-5da7-4403-8623-598d8a5980ac",
   "metadata": {},
   "source": [
    "# ResumeEvaluator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09a4ebf7-a758-4c8f-8c9b-a94f55c5066e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResumeEvaluator class defined\n"
     ]
    }
   ],
   "source": [
    "class ResumeEvaluator:\n",
    "    \"\"\"LLM-based evaluator for resume parsing quality.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str):\n",
    "        if not api_key:\n",
    "            raise ValueError(\"API key required for evaluation\")\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        \n",
    "    def evaluate(self, original_text: str, extracted_data: Dict) -> Dict:\n",
    "        \"\"\"Evaluate extraction quality using LLM judge.\"\"\"\n",
    "        \n",
    "        prompt = f\"\"\"Evaluate this resume parsing extraction:\n",
    "\n",
    "RESUME TEXT (first 800 chars):\n",
    "{original_text[:5000]}\n",
    "\n",
    "EXTRACTED DATA:\n",
    "{json.dumps(extracted_data, indent=2)}\n",
    "\n",
    "For each field, determine if extraction is:\n",
    "- \"correct\": Perfectly extracted and accurate\n",
    "- \"incorrect\": Missing, wrong, or completely inaccurate  \n",
    "- \"partial\": Extracted but incomplete or minor issues\n",
    "\n",
    "Also provide overall extraction quality (excellent/good/fair/poor) and brief reasoning.\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert resume parsing evaluator.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                response_format={\n",
    "                    \"type\": \"json_schema\", \n",
    "                    \"json_schema\": Config.EVALUATION_SCHEMA\n",
    "                },\n",
    "                temperature=0.1,\n",
    "                max_tokens=1000\n",
    "            )\n",
    "            \n",
    "            return json.loads(response.choices[0].message.content)\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Evaluation failed: {e}\"}\n",
    "\n",
    "print(\"ResumeEvaluator class defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a103b1c0-0ddf-432f-a330-8c7d9a96a218",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2003b67e-966a-470b-a353-1b741b7ad14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def calculate_metrics(results: List[Dict]) -> Dict:\n",
    "    \"\"\"Calculate simple performance metrics.\"\"\"\n",
    "    if not results:\n",
    "        return {}\n",
    "    \n",
    "    successful = [r for r in results if r.get(\"success\")]\n",
    "    total = len(results)\n",
    "    \n",
    "    # Basic metrics\n",
    "    metrics = {\n",
    "        \"total_files\": total,\n",
    "        \"successful_parses\": len(successful),\n",
    "        \"success_rate\": len(successful) / total * 100 if total > 0 else 0,\n",
    "        \"avg_skills_per_resume\": sum(\n",
    "            len(r.get(\"parsed_data\", {}).get(\"skills\", [])) \n",
    "            for r in successful\n",
    "        ) / len(successful) if successful else 0\n",
    "    }\n",
    "    \n",
    "    # Evaluation metrics\n",
    "    evaluations = [\n",
    "        r.get(\"evaluation\") for r in results \n",
    "        if r.get(\"evaluation\") and not r.get(\"evaluation\", {}).get(\"error\")\n",
    "    ]\n",
    "    \n",
    "    if evaluations:\n",
    "        name_correct = sum(1 for e in evaluations if e.get(\"name_status\") == \"correct\")\n",
    "        email_correct = sum(1 for e in evaluations if e.get(\"email_status\") == \"correct\")\n",
    "        skills_correct = sum(1 for e in evaluations if (e.get(\"skills_status\") == \"correct\" or e.get(\"skills_status\") == \"partial\"))\n",
    "        \n",
    "        # Calculate overall quality distribution\n",
    "        quality_distribution = {\"excellent\": 0, \"good\": 0, \"fair\": 0, \"poor\": 0}\n",
    "        for evaluation in evaluations:\n",
    "            quality = evaluation.get(\"extraction_quality\", \"poor\")\n",
    "            quality_distribution[quality] += 1\n",
    "        \n",
    "        metrics.update({\n",
    "            \"evaluation_count\": len(evaluations),\n",
    "            \"name_accuracy\": name_correct / len(evaluations) * 100,\n",
    "            \"email_accuracy\": email_correct / len(evaluations) * 100,\n",
    "            \"skills_accuracy\": skills_correct / len(evaluations) * 100,\n",
    "            \"overall_quality_distribution\": quality_distribution\n",
    "        })\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def print_summary(metrics: Dict):\n",
    "    \"\"\"Print benchmark summary.\"\"\"\n",
    "    print(\"\\nBENCHMARK SUMMARY\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Files processed: {metrics.get('total_files', 0)}\")\n",
    "    print(f\"Success rate: {metrics.get('success_rate', 0):.1f}%\")\n",
    "    print(f\"Avg skills per resume: {metrics.get('avg_skills_per_resume', 0):.1f}\")\n",
    "    \n",
    "    if metrics.get('evaluation_count'):\n",
    "        print(f\"\\nEVALUATION RESULTS ({metrics['evaluation_count']} files):\")\n",
    "        print(f\"Name accuracy: {metrics.get('name_accuracy', 0):.1f}%\")\n",
    "        print(f\"Email accuracy: {metrics.get('email_accuracy', 0):.1f}%\")\n",
    "        print(f\"Skills accuracy: {metrics.get('skills_accuracy', 0):.1f}%\")\n",
    "        \n",
    "        # Display overall quality distribution\n",
    "        quality_dist = metrics.get('overall_quality_distribution')\n",
    "        if quality_dist:\n",
    "            print(f\"\\nOVERALL EXTRACTION QUALITY:\")\n",
    "            for quality in ['excellent', 'good', 'fair', 'poor']:\n",
    "                count = quality_dist.get(quality, 0)\n",
    "                percentage = (count / metrics['evaluation_count'] * 100) if metrics['evaluation_count'] > 0 else 0\n",
    "                print(f\"  {quality.capitalize()}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "print(\"Helper functions defined\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77745f6-159e-49de-a9fd-92524482799b",
   "metadata": {},
   "source": [
    "# Main analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93822cb4-7252-4372-9b58-1f669860c87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "703ed23ac9f24aa7b3ecc31183db2803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f396c0c643d64bbb8f0a914c8c32093d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 resume files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed ATS basic HR resume.docx: Name=True, Email=True, Skills=1\n",
      "Processed ATS Bold accounting resume.pdf: Name=True, Email=True, Skills=6\n",
      "Processed ATS bold HR resume.docx: Name=True, Email=True, Skills=1\n",
      "Processed ATS classic HR resume.docx: Name=True, Email=True, Skills=15\n",
      "Processed ATS healthcare resume.pdf: Name=True, Email=True, Skills=5\n",
      "Processed ATS office manager resume.pdf: Name=True, Email=True, Skills=7\n",
      "Processed ATS simple classic resume.pdf: Name=True, Email=True, Skills=6\n",
      "Processed ATS stylish accounting resume.docx: Name=True, Email=True, Skills=6\n",
      "Processed Extended ATS healthcare resume.docx: Name=True, Email=False, Skills=1\n",
      "Processed Simple ATS healthcare resume.pdf: Name=True, Email=True, Skills=12\n",
      "\n",
      "BENCHMARK SUMMARY\n",
      "========================================\n",
      "Files processed: 10\n",
      "Success rate: 100.0%\n",
      "Avg skills per resume: 6.0\n",
      "\n",
      "EVALUATION RESULTS (10 files):\n",
      "Name accuracy: 100.0%\n",
      "Email accuracy: 90.0%\n",
      "Skills accuracy: 100.0%\n",
      "\n",
      "OVERALL EXTRACTION QUALITY:\n",
      "  Excellent: 1 (10.0%)\n",
      "  Good: 6 (60.0%)\n",
      "  Fair: 3 (30.0%)\n",
      "  Poor: 0 (0.0%)\n",
      "\n",
      "Results saved to: output\\resume_parsing_results_20250829_103633.json\n"
     ]
    }
   ],
   "source": [
    "# Initialize components\n",
    "parser = ResumeParser()\n",
    "evaluator = ResumeEvaluator(Config.API_KEY) if Config.API_KEY else None\n",
    "\n",
    "# Setup directories\n",
    "data_dir = Path(Config.DATA_DIR)\n",
    "output_dir = Path(Config.OUTPUT_DIR)\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "if not data_dir.exists():\n",
    "    print(f\"Error: Data directory '{data_dir}' not found\")\n",
    "else:\n",
    "    # Find resume files\n",
    "    resume_files = [f for f in data_dir.iterdir() \n",
    "                    if f.suffix.lower() in ['.pdf', '.docx', '.txt']]\n",
    "    \n",
    "    print(f\"Found {len(resume_files)} resume files\")\n",
    "    \n",
    "    # Process files\n",
    "    results = []\n",
    "    \n",
    "    for file_path in resume_files:\n",
    "        try:\n",
    "            # Parse resume\n",
    "            parsed_data = parser.parse_resume(file_path)\n",
    "            \n",
    "            result = {\n",
    "                \"file_name\": file_path.name,\n",
    "                \"parsed_data\": parsed_data,\n",
    "                \"success\": bool(parsed_data[\"name\"] or parsed_data[\"email\"] or parsed_data[\"skills\"])\n",
    "            }\n",
    "            \n",
    "            # Evaluate if API key available\n",
    "            if evaluator:\n",
    "                original_text = parser.extract_text(file_path)\n",
    "                evaluation = evaluator.evaluate(original_text, parsed_data)\n",
    "                result[\"evaluation\"] = evaluation\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            print(f\"Processed {file_path.name}: Name={bool(parsed_data['name'])}, Email={bool(parsed_data['email'])}, Skills={len(parsed_data['skills'])}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path.name}: {e}\")\n",
    "            results.append({\"file_name\": file_path.name, \"error\": str(e), \"success\": False})\n",
    "    \n",
    "    # Calculate and display metrics\n",
    "    metrics = calculate_metrics(results)\n",
    "    print_summary(metrics)\n",
    "    \n",
    "    # Save results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_file = output_dir / f\"resume_parsing_results_{timestamp}.json\"\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump({\"results\": results, \"metrics\": metrics}, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\nResults saved to: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775a9549-8ee4-44c0-9873-c89045bb8f91",
   "metadata": {},
   "source": [
    "# Detailed result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5068db0d-c50f-440f-8d9c-9d042647728a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DETAILED QUALITY ANALYSIS\n",
      "==================================================\n",
      "\n",
      "EXCELLENT QUALITY (1 files):\n",
      "  File: ATS simple classic resume.pdf\n",
      "    Name: correct, Email: correct, Skills: correct\n",
      "    Reasoning: All fields were accurately extracted with no missing or incorrect information. The name, email, and skills are all correctly identified and formatted....\n",
      "\n",
      "\n",
      "GOOD QUALITY (6 files):\n",
      "  File: ATS Bold accounting resume.pdf\n",
      "    Name: correct, Email: correct, Skills: partial\n",
      "    Reasoning: The name and email were extracted accurately. The skills list is mostly correct but includes a minor formatting issue with 'State & federal tax codes' which should be 'State & federal tax codes' without the extra space. Additionally, 'GAAP' was mentioned in the resume but not included in the extracted skills list, which is a significant omission. Overall, the extraction is good but could be improved with complete skills representation....\n",
      "\n",
      "  File: ATS classic HR resume.docx\n",
      "    Name: correct, Email: correct, Skills: partial\n",
      "    Reasoning: The name and email were extracted accurately. However, the skills list is incomplete and contains some inaccuracies, such as including 'project', 'management', and 'software' as separate skills instead of as part of 'project management software'. Additionally, 'communication' is mentioned twice in different forms. Overall, the extraction is good but could be improved for the skills section....\n",
      "\n",
      "  File: ATS healthcare resume.pdf\n",
      "    Name: correct, Email: correct, Skills: partial\n",
      "    Reasoning: The name and email were extracted correctly. However, the skills list is missing 'Chronic disease management' and 'Health promotion', and incorrectly includes 'Trail running', which is a hobby, not a skill. Overall, the extraction is good but not perfect due to the skills inaccuracies....\n",
      "\n",
      "  File: ATS office manager resume.pdf\n",
      "    Name: correct, Email: correct, Skills: partial\n",
      "    Reasoning: The name and email were extracted accurately. However, the skills list includes 'LinkedIn', which is not a skill but a platform, making the skills extraction partial. Overall, the extraction quality is good due to the accurate name and email....\n",
      "\n",
      "  File: ATS stylish accounting resume.docx\n",
      "    Name: correct, Email: correct, Skills: partial\n",
      "    Reasoning: The name and email were extracted accurately. However, the skills list is incomplete as it does not capture all relevant skills mentioned in the resume text, such as 'accountant' and 'public accounting experience', which are significant to the candidate's profile....\n",
      "\n",
      "  File: Simple ATS healthcare resume.pdf\n",
      "    Name: correct, Email: correct, Skills: partial\n",
      "    Reasoning: The name and email were extracted accurately. However, the skills list contains duplicates and some phrases that are not standalone skills (e.g., 'attention to detail' is split into two entries). This affects the overall quality of the skills extraction, making it partial....\n",
      "\n",
      "\n",
      "FAIR QUALITY (3 files):\n",
      "  File: ATS basic HR resume.docx\n",
      "    Name: correct, Email: correct, Skills: partial\n",
      "    Reasoning: The name and email were extracted correctly. However, the skills extraction is incomplete as it only includes 'natural leader' and misses other relevant skills that could be inferred from the resume text, such as 'communication' and 'leadership'. Overall, the extraction quality is fair due to the missing skills....\n",
      "\n",
      "  File: ATS bold HR resume.docx\n",
      "    Name: correct, Email: correct, Skills: partial\n",
      "    Reasoning: The name and email were extracted correctly. However, the skills extraction is incomplete as it only includes 'natural leader' without capturing any other relevant skills or competencies that may have been implied in the resume text....\n",
      "\n",
      "  File: Extended ATS healthcare resume.docx\n",
      "    Name: correct, Email: incorrect, Skills: correct\n",
      "    Reasoning: The name was extracted correctly, and the skill 'Phlebotomist' was accurately identified. However, the email field is missing entirely, which is a significant omission. Overall, the extraction quality is fair due to the missing email....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Detailed Quality Analysis (Optional)\n",
    "def display_detailed_quality_analysis(results: List[Dict]):\n",
    "    \"\"\"Display detailed analysis of extraction quality by file.\"\"\"\n",
    "    \n",
    "    evaluations = [\n",
    "        r for r in results \n",
    "        if r.get(\"evaluation\") and not r.get(\"evaluation\", {}).get(\"error\")\n",
    "    ]\n",
    "    \n",
    "    if not evaluations:\n",
    "        print(\"No evaluation data available for detailed analysis\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nDETAILED QUALITY ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Group by quality level\n",
    "    quality_groups = {\"excellent\": [], \"good\": [], \"fair\": [], \"poor\": []}\n",
    "    \n",
    "    for result in evaluations:\n",
    "        evaluation = result[\"evaluation\"]\n",
    "        quality = evaluation.get(\"extraction_quality\", \"poor\")\n",
    "        quality_groups[quality].append({\n",
    "            \"file\": result[\"file_name\"],\n",
    "            \"name_status\": evaluation.get(\"name_status\", \"unknown\"),\n",
    "            \"email_status\": evaluation.get(\"email_status\", \"unknown\"),\n",
    "            \"skills_status\": evaluation.get(\"skills_status\", \"unknown\"),\n",
    "            \"reasoning\": evaluation.get(\"overall_reasoning\", \"No reasoning provided\")\n",
    "        })\n",
    "    \n",
    "    # Display each quality group\n",
    "    for quality in ['excellent', 'good', 'fair', 'poor']:\n",
    "        files = quality_groups[quality]\n",
    "        if files:\n",
    "            print(f\"\\n{quality.upper()} QUALITY ({len(files)} files):\")\n",
    "            for file_data in files:\n",
    "                print(f\"  File: {file_data['file']}\")\n",
    "                print(f\"    Name: {file_data['name_status']}, Email: {file_data['email_status']}, Skills: {file_data['skills_status']}\")\n",
    "                print(f\"    Reasoning: {file_data['reasoning'][:2500]}{'...' if len(file_data['reasoning']) > 100 else ''}\")\n",
    "                print()\n",
    "\n",
    "# Run detailed analysis (optional)\n",
    "if 'results' in locals():\n",
    "    display_detailed_quality_analysis(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8135c1c-5e32-479f-bd06-8cd50f0a1886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b0cfca-c383-4141-bb20-9d162505b5e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
